"""GRPO entrypoint using the online dataset."""

import json
import os

import ray
from omegaconf import OmegaConf

from verl.single_controller.ray import RayWorkerGroup
from verl.trainer.config import PPOConfig
from verl.trainer.ray_trainer import RayPPOTrainer, ResourcePoolManager, Role
from verl.utils.tokenizer import get_processor, get_tokenizer
from verl.workers.fsdp_workers import FSDPWorker
from verl.workers.reward import AutoRewardManager

from .online_dataloader import create_online_dataloader


@ray.remote(num_cpus=1)
class Runner:
    """A runner for RL training."""

    def run(self, config: PPOConfig):
        # breakpoint()
        print(json.dumps(config.to_dict(), indent=2))

        tokenizer = get_tokenizer(
            config.worker.actor.model.model_path,
            override_chat_template=config.data.override_chat_template,
            trust_remote_code=config.worker.actor.model.trust_remote_code,
            use_fast=True,
        )
        processor = get_processor(
            config.worker.actor.model.model_path,
            override_chat_template=config.data.override_chat_template,
            trust_remote_code=config.worker.actor.model.trust_remote_code,
            use_fast=True,
        )

        ray_worker_group_cls = RayWorkerGroup
        role_worker_mapping = {
            Role.ActorRolloutRef: ray.remote(FSDPWorker),
            Role.Critic: ray.remote(FSDPWorker),
        }
        global_pool_id = "global_pool"
        resource_pool_spec = {
            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
        }
        mapping = {
            Role.ActorRolloutRef: global_pool_id,
            Role.Critic: global_pool_id,
        }
        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)

        RemoteRewardManager = ray.remote(AutoRewardManager).options(num_cpus=config.worker.reward.num_cpus)
        reward_fn = RemoteRewardManager.remote(config.worker.reward, tokenizer)
        val_reward_fn = RemoteRewardManager.remote(config.worker.reward, tokenizer)

        train_dataloader, val_dataloader = create_online_dataloader(config.data, tokenizer, processor)

        trainer = RayPPOTrainer(
            config=config,
            tokenizer=tokenizer,
            processor=processor,
            train_dataloader=train_dataloader,
            val_dataloader=val_dataloader,
            role_worker_mapping=role_worker_mapping,
            resource_pool_manager=resource_pool_manager,
            ray_worker_group_cls=ray_worker_group_cls,
            reward_fn=reward_fn,
            val_reward_fn=val_reward_fn,
        )
        trainer.init_workers()
        trainer.fit()


def main():
    cli_args = OmegaConf.from_cli()
    default_config = OmegaConf.structured(PPOConfig())

    if hasattr(cli_args, "config"):
        config_path = cli_args.pop("config", None)
        file_config = OmegaConf.load(config_path)
        default_config = OmegaConf.merge(default_config, file_config)

    ppo_config = OmegaConf.merge(default_config, cli_args)
    ppo_config: PPOConfig = OmegaConf.to_object(ppo_config)
    ppo_config.deep_post_init()

    # if not ray.is_initialized():
        # runtime_env = {
        #     "env_vars": {
        #         "TOKENIZERS_PARALLELISM": "true",
        #         "NCCL_DEBUG": "WARN",
        #         "VLLM_LOGGING_LEVEL": "WARN",
        #         "TORCH_NCCL_AVOID_RECORD_STREAMS": "1",
        #         "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:False",
        #         "CUDA_DEVICE_MAX_CONNECTIONS": "1",
        #         "VLLM_ALLREDUCE_USE_SYMM_MEM": "0",
        #         "VLLM_USE_V1": "0",
        #         "working_dir": "/tmp/shared-storage/lishichao/EasyR1/",
        #         "ORACLE_REDIS_URL": os.getenv("ORACLE_REDIS_URL", ppo_config.data.ORACLE_REDIS_URL),
        #         "ORACLE_QUEUE_KEY": os.getenv("ORACLE_QUEUE_KEY", ppo_config.data.ORACLE_QUEUE_KEY),
        #         "ORACLE_REDIS_TIMEOUT": os.getenv("ORACLE_REDIS_TIMEOUT", ""),
        #         "ORACLE_REDIS_MAX_RETRIES": os.getenv("ORACLE_REDIS_MAX_RETRIES", ""),
        #         "ORACLE_REDIS_RETRY_SLEEP": os.getenv("ORACLE_REDIS_RETRY_SLEEP", ""),
        #         "ORACLE_FALLBACK_SAMPLE_PROB": os.getenv("ORACLE_FALLBACK_SAMPLE_PROB", ""),
        #         "ORACLE_DATASET_NUM_WORKERS": os.getenv("ORACLE_DATASET_NUM_WORKERS", ""),
        #         "ORACLE_WEAKNESS_KEY": os.getenv("ORACLE_WEAKNESS_KEY", ""),
        #         "ORACLE_WEAKNESS_TOPK": os.getenv("ORACLE_WEAKNESS_TOPK", ""),
        #         "ORACLE_WEAKNESS_THRESHOLD": os.getenv("ORACLE_WEAKNESS_THRESHOLD", ""),
        #     }
        # }
        # ray.init(local_mode=True, runtime_env=runtime_env)

    runner = Runner.remote()
    ray.get(runner.run.remote(ppo_config))

    if ppo_config.trainer.ray_timeline is not None:
        ray.timeline(filename=ppo_config.trainer.ray_timeline)


if __name__ == "__main__":
    main()
